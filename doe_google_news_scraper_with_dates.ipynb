{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32fa2be",
   "metadata": {},
   "source": [
    "# DOE Google News Scraper with Accurate Dates\n",
    "This notebook scrapes Google News for DOE press releases (2020‚Äì2025), classifies them by topic, and extracts actual article publication dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7291b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f08cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: classify category based on title\n",
    "def classify_category(title):\n",
    "    title = title.lower()\n",
    "    if any(k in title for k in [\"policy\", \"rule\", \"licens\", \"regulation\", \"act\", \"executive\", \"approval\", \"roadmap\", \"nrc\"]):\n",
    "        return \"policy\"\n",
    "    elif any(k in title for k in [\"reactor\", \"fusion\", \"smr\", \"deployment\", \"technology\", \"microreactor\", \"grant\", \"terrapower\"]):\n",
    "        return \"tech\"\n",
    "    elif any(k in title for k in [\"anniversary\", \"hiroshima\", \"fukushima\"]):\n",
    "        return \"anniversary\"\n",
    "    elif any(k in title for k in [\"protest\", \"lawsuit\", \"opposition\", \"activist\"]):\n",
    "        return \"protest\"\n",
    "    elif any(k in title for k in [\"leak\", \"shutdown\", \"disaster\", \"accident\", \"alert\"]):\n",
    "        return \"disaster\"\n",
    "    elif any(k in title for k in [\"france\", \"russia\", \"china\", \"ukraine\", \"international\"]):\n",
    "        return \"international\"\n",
    "    return \"other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google News Scraper with accurate date extraction\n",
    "def google_news_scraper(query, start_year, end_year, pause=2, max_pages=5):\n",
    "    base_url = \"https://www.google.com/search?q={query}&tbm=nws&tbs=cdr:1,cd_min:{start},cd_max:{end}&start={page}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "    query_encoded = urllib.parse.quote_plus(query)\n",
    "    articles = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        start_date = f\"01/01/{year}\"\n",
    "        end_date = f\"12/31/{year}\"\n",
    "        seen_titles = set()\n",
    "\n",
    "        for page_num in range(max_pages):\n",
    "            page = page_num * 10\n",
    "            url = base_url.format(query=query_encoded, start=start_date, end=end_date, page=page)\n",
    "            print(f\"üîç Scraping: {url}\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Failed to fetch page {page_num+1} for {year}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            found = 0\n",
    "\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                h3 = a.find(\"h3\")\n",
    "                if h3:\n",
    "                    title = h3.get_text(strip=True)\n",
    "                    if title in seen_titles:\n",
    "                        continue\n",
    "                    seen_titles.add(title)\n",
    "                    link = a['href']\n",
    "                    if link.startswith(\"/url?q=\"):\n",
    "                        cleaned_link = link.split(\"/url?q=\")[1].split(\"&\")[0]\n",
    "\n",
    "                        # Try to get the date from a nearby <span>\n",
    "                        span = a.find_next(\"span\")\n",
    "                        try:\n",
    "                            parsed_date = parser.parse(span.get_text(strip=True), fuzzy=True).date().isoformat()\n",
    "                        except Exception:\n",
    "                            parsed_date = f\"{year}-01-01\"  # fallback if parsing fails\n",
    "\n",
    "                        articles.append({\n",
    "                            \"date\": parsed_date,\n",
    "                            \"label\": title,\n",
    "                            \"category\": classify_category(title),\n",
    "                            \"url\": cleaned_link\n",
    "                        })\n",
    "                        found += 1\n",
    "\n",
    "            print(f\"‚úÖ Year {year}, Page {page_num+1}: {found} articles found\")\n",
    "            time.sleep(pause)\n",
    "\n",
    "    return pd.DataFrame(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ee380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and save\n",
    "df = google_news_scraper(\"site:energy.gov press release\", 2020, 2025, pause=2, max_pages=5)\n",
    "df.to_csv(\"doe_press_releases_2020_2025_google_news.csv\", index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
